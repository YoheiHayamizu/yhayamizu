---
layout: post
title: DKPROMPT - Domain Knowledge Prompting Vision-Language Models for Open-World Planning
date: 2025-02-01 09:00:00 -0500
tags:
- Publications
- Vision-Language Models
- Robot Planning
- Workshop
---

# Publication
Our paper, titled **[DKPROMPT: Domain Knowledge Prompting Vision-Language Models for Open-World Planning](https://openreview.net/pdf?id=ANXjmdDQyV)**, was accepted at the **AAAI LM4Plan Workshop 2025** and previously presented at **CVPR EAI Workshop 2024**.

This work introduces a novel approach to enhance vision-language models with domain-specific knowledge for improved robot planning in open-world scenarios.

## Research Overview
Vision-language models show great promise for robot planning, but often lack domain-specific knowledge crucial for real-world applications. DKPROMPT addresses this limitation by incorporating structured domain knowledge into the prompting process.

## Key Features
- Domain knowledge integration framework
- Enhanced vision-language model performance
- Open-world planning capabilities
- Comprehensive experimental validation

## Impact
This research contributes to making robots more capable of understanding and operating in complex, real-world environments by bridging the gap between general-purpose models and domain-specific requirements.

**Authors:** Xiaohan Zhang, Zainab Altaweel*, Yohei Hayamizu*, Yan Ding, Saeid Amiri, Hao Yang, Andy Kaminski, Chad Esselink, and Shiqi Zhang (*Equal contribution)

**Venues:**
- AAAI LM4Plan Workshop 2025
- CVPR EAI Workshop 2024

[Paper](https://openreview.net/pdf?id=ANXjmdDQyV) | [Project Website](https://dkprompt.github.io/) | [arXiv](https://arxiv.org/abs/2406.17659)