---
layout: post
title: "DKPROMPT: Domain Knowledge Prompting Vision-Language Models for Open-World Planning"
date: 2024-12-31 09:00:00 -0400
tags:
- Publications
- Vision-Language Models
- Robot Planning
- Workshop
---

# Publication Update
Our paper, titled **[DKPROMPT: Domain Knowledge Prompting Vision-Language Models for Open-World Planning](https://arxiv.org/abs/2406.17659)**, was accepted to **CVPR EAI Workshop 2024**!

## Research Overview
Vision-language models show great promise for robot planning, but often lack domain-specific knowledge crucial for real-world applications. DKPROMPT addresses this limitation by incorporating structured domain knowledge into the prompting process.

## Key Innovation
Our framework enhances vision-language models with domain-specific knowledge, enabling more effective robot planning in open-world scenarios. This bridges the gap between general-purpose AI models and the specialized requirements of real-world robotic applications.

## Key Features
- Domain knowledge integration framework
- Enhanced vision-language model performance
- Open-world planning capabilities
- Comprehensive experimental validation

## Impact and Recognition
The acceptance at multiple prestigious venues (CVPR EAI Workshop and AAAI LM4Plan Workshop) demonstrates the broad impact and relevance of this work across computer vision and AI planning communities.

**Authors:** Xiaohan Zhang, Zainab Altaweel*, Yohei Hayamizu*, Yan Ding, Saeid Amiri, Hao Yang, Andy Kaminski, Chad Esselink, and Shiqi Zhang (*Equal contribution)

**Venues:**
- CVPR EAI Workshop 2024

[arXiv Paper](https://arxiv.org/abs/2406.17659) | [Project Website](https://dkprompt.github.io/)
